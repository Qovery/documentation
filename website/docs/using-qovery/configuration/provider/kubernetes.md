---
last_modified_on: "2023-12-27"
title: "Kubernetes"
description: "Learn how to install and configure Qovery on your own Kubernetes cluster (BYOK) / Self-managed Kubernetes cluster"
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

import Steps from '@site/src/components/Steps';
import Alert from '@site/src/components/Alert';
import Assumptions from '@site/src/components/Assumptions';

<Alert type="warning">

Installing Qovery on your Kubernetes cluster is currently in beta. [You need to request your access here](https://www.qovery.com/solutions/bring-your-own-kubernetes).

</Alert>

<Alert type="info">

This section is for Kubernetes power-users. If you are not familiar with Kubernetes, we recommend you to use Qovery on a Managed Kubernetes cluster on [AWS][docs.using-qovery.configuration.cloud-service-provider.amazon-web-services], [GCP][docs.using-qovery.configuration.cloud-service-provider.google-cloud-platform], [Scaleway][docs.using-qovery.configuration.cloud-service-provider.microsoft-azure], or contact us.

</Alert>

Qovery Self-Managed (or BYOK: Bring Your Own Kubernetes) is a self-hosted version of Qovery. It allows you to install Qovery on your own Kubernetes cluster.
Read [this article](https://www.qovery.com/blog/kubernetes-managed-by-qovery-vs-self-managed-byok) to better understand the difference with the Managed Kubernetes by Qovery. In a nutshell, Qovery Managed/BYOK is for Kubernetes experts who want to manage their own Kubernetes cluster.
In this version, Qovery does not manage the Kubernetes cluster for you.

This page explains how to install and configure Qovery on your Kubernetes cluster. If you are looking for a quick step-by-step guide, please follow the [Kubernetes guide][guides.provider.guide-kubernetes].

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/using-qovery/configuration/provider/kubernetes.md.erb
-->

## Components

<p align="center">
  <img src="/img/qovery_byok_how_it_works.jpg" alt="How Qovery works with Self Managed Kubernetes cluster" />
</p>

They are two types of components:

Qovery components:
- Qovery Control Plane: the Qovery Control Plane is the brain of Qovery. It is responsible for managing your applications and providing the API to interact with Qovery.
- Qovery Cluster Agent (mandatory): the Qovery Cluster Agent is responsible for securely forwarding logs and metrics from your Kubernetes cluster to Qovery control plane.
- Qovery Shell Agent (mandatory): the Qovery Shell Agent is responsible for giving you a secure remote shell access to your Kubernetes pods if you need it. E.g. when using `qovery shell` command.
- Qovery Engine (optional): the Qovery Engine is responsible for managing your applications deployment on your Kubernetes cluster. It can be used Qovery side or is installed on your Kubernetes cluster.

Third-party components:
- NGINX Ingress Controller (optional)
- External DNS (optional)
- Loki (optional)
- Promtail (optional)
- Cert Manager (optional)
- ...

You can chose what you want to install and manage, and you will have a description of what services are used, and responsible for. You can disable them if you don't want to use them. And you can even install other components if you want to.

## What's the requirements?

Qovery requires a Kubernetes cluster with the following requirements:

- Kubernetes version 1.26 or higher
- Helm version 3.0 or higher
- 2 CPU
- 4 GB RAM
- 20 GB disk space
- Being able to access to the Internet
- A private registry


Here are some examples of Kubernetes distributions that can be used with Qovery. **This is a non exhaustive list**.

<Alert type="danger">

Theses examples are not recommendations! Simply examples of what can be installed in the fastest way.

</Alert>

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"eks"}
  placeholder="Select a cloud provider"
  select={false}
  size={null}
  values={[
    {"group":"k8s","label":"AWS - EKS","value":"eks"},
    {"group":"k8s","label":"GCP - GKE","value":"gke"},
    {"group":"k8s","label":"Scaleway - Kapsule","value":"kapsule"},
    {"group":"k8s","label":"Demo (K3D)","value":"demo"}
    ]}>

<TabItem value="eks">

To create a Kubernetes cluster on AWS, the simplest way is to use [eksctl binary](https://eksctl.io/installation/). For example:

```bash
eksctl create cluster --region=us-east-2 --zones=us-east-2a,us-east-2b,us-east-2d
```
</TabItem>

<TabItem value="gke">

To create a Kubernetes cluster on GCP, the simplest way is to use [gcloud binary](https://cloud.google.com/sdk/docs/install). For example:

```bash
gcloud beta container --project "qovery-gcp-tests" \
  clusters create-auto "qovery-test" \
  --region "us-east5" \
  --release-channel "stable" \
  --network "projects/qovery-gcp-tests/global/networks/default" \
  --subnetwork "projects/qovery-gcp-tests/regions/us-east5/subnetworks/default"
  --cluster-ipv4-cidr "/16"
  --services-ipv4-cidr "10.0.0.0/16"
```
</TabItem>

<TabItem value="kapsule">

To create a Kubernetes cluster on Scaleway, the simplest way is to use [scw binary](https://github.com/scaleway/scaleway-cli). For example:

```bash
scw k8s cluster create name=qovery-test
scw k8s pool create cluster-id=<id-of-your-cluster> name=pool node-type=GP1_XL size=3
```

You can find the [complete documentation here](https://www.scaleway.com/en/docs/containers/kubernetes/api-cli/creating-managing-kubernetes-lifecycle-cliv2/).

</TabItem>

<TabItem value="demo">

Here is an example with K3d to deploy a local Kubernetes cluster (you can use k3s or any other Kubernetes distribution):

```bash
k3d cluster create --image rancher/k3s:v1.26.11-k3s2 --k3s-arg "--disable=traefik,metrics-server@server:0" \
-v $(pwd)/registry_bin:/var/lib/rancher/credentialprovider/bin@server:0  \
-v $(pwd)/config.yaml:/var/lib/rancher/credentialprovider/config.yaml@server:0
```

Note: please take a look at the registry information below to understand why we need to mount the registry folder.

</TabItem>
</Tabs>

## Private registry

Qovery requires a private registry to store built images and mirror containers in order to reduce potential images deletion by 3rd party, while you still need them ([more info here][docs.using-qovery.deployment.image-mirroring]).

<p align="center">
  <img src="/img/configuration/provider/kubelet-credential-providers-plugin.png" alt="Kubelet Credential Providers" />
</p>

To do so, Qovery advise to use [Kubelet Credential Provider](https://kubernetes.io/blog/2022/12/22/kubelet-credential-providers/) as it's transparent for developers.

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"cpor"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[{"group":"Registry","label":"Cloud Provider Own Registry","value":"cpor"},{"group":"Registry","label":"ECR","value":"ecr"}]}>

<TabItem value="cpor">

If you're running Qovery Self-Managed version, and you are going to use the registry from the cloud provider itself, you don't have anything to do. The cloud providers already manage this part for you.

</TabItem>

<TabItem value="ecr">

If you want to use ECR on a non-EKS cluster, you will need to install the ECR Credential Provider on your Kubernetes cluster.

You have to create an IAM user with the following policy, and generate an access key:
```json
{
    "Statement": [
        {
            "Action": [
                "ecr:*",
            ],
            "Effect": "Allow",
            "Resource": "*"
        }
    ],
    "Version": "2012-10-17"
}
```

Then we create a `config.yaml` file to configure the ECR credential provider, where you should set the AWS credentials previously generated:
```yaml
apiVersion: kubelet.config.k8s.io/v1
kind: CredentialProviderConfig
providers:
  - name: ecr-credential-provider
    matchImages:
      - "*.dkr.ecr.*.amazonaws.com"
      - "*.dkr.ecr.*.amazonaws.com.cn"
      - "*.dkr.ecr-fips.*.amazonaws.com"
      - "*.dkr.ecr.us-iso-east-1.c2s.ic.gov"
      - "*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov"
    defaultCacheDuration: "12h"
    apiVersion: credentialprovider.kubelet.k8s.io/v1
    env:
      - name: AWS_ACCESS_KEY_ID
        value: xxx
      - name: AWS_DEFAULT_REGION
        value: xxx
      - name: AWS_SECRET_ACCESS_KEY
        value: xxx
```

<Alert type="info">

Depending on your Kubernetes installation (cloud provider, on premise...) please refer to the official documentation to deploy the credential provider.

</Alert>

<details><summary>Example with K3d</summary>

Here is an example with K3d to deploy a local Kubernetes cluster with the ECR credential provider.

We first create the prerequired folders and file for the binary:
```
mkdir -p registry_bin
touch registry_bin/ecr-credential-provider
chmod 755 registry_bin/ecr-credential-provider
```
Note: the ecr-credential-provider binary should be present for k3s to start. We will build it later.

Now we can run a local Kubernetes cluster (update the path to `config.yaml` file, and the Kubernetes [image tag version](https://hub.docker.com/r/rancher/k3s/tags)):
```bash
k3d cluster create --image rancher/k3s:v1.26.11-k3s2 --k3s-arg "--disable=traefik,metrics-server@server:0" \
-v $(pwd)/registry_bin:/var/lib/rancher/credentialprovider/bin@server:0  \
-v $(pwd)/config.yaml:/var/lib/rancher/credentialprovider/config.yaml@server:0
```

</details><br />

Once the Credential provider configuration has been deployed, we'll build the binary and deploy it on the cluster (note: it has to be present on all worker nodes).
Simply deploy this job which will do the work:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: cloud-provider-repository-binary-builder
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: ecr-credential-builder
        image: alpine:3.18
        command:
          - /bin/sh
          - -c
          - |
            apk add -U ca-certificates tar zstd tzdata go git
            git clone https://github.com/kubernetes/cloud-provider-aws.git
            cd cloud-provider-aws/cmd/ecr-credential-provider
            CGO_ENABLED=0 go build -mod=readonly .
            chmod 755 ecr-credential-provider
            mkdir -p /mnt/host/var/lib/rancher/credentialprovider/bin/
            cp ecr-credential-provider /mnt/host/var/lib/rancher/credentialprovider/bin/
        volumeMounts:
        - mountPath: /mnt/host
          name: host
      volumes:
      - hostPath:
          path: /
          type: ""
        name: host
```

You can now move on the Qovery Helm deployment.

</TabItem>
</Tabs>

## Install Qovery

<Steps headingDepth={3}>

<ol>

<li>

Install [Helm][urls.helm] command line tool.

</li>

<li>

Add Qovery Helm repository.

```bash
helm repo add qovery https://helm.qovery.com
helm repo update
```

</li>

<li>

Login to the Qovery console, create a cluster until it's asked to save informations in the `values.yaml` file.

</li>

<li>

You will find several `values.yaml` files. Depending on you need, select the one you want and update the configuration accordingly:
* `values-demo.yaml`: this version is to locally test with k3s or minikube
* `values-<provider-name>.yaml`: find versions made for some providers

<Alert type="info">

Note: The provided values files are examples. There is no restriction where Qovery can be deployed. Feel free to copy or use an existing example and adapt it to your needs.

</Alert>

Here is an example of how the chart works:
<details><summary>values-demo.yaml</summary>

```yaml
## Services you can enable or disable
services:
  qovery:
    qovery-cluster-agent:
      enabled: true
    qovery-shell-agent:
      enabled: true
  ingress:
    ingress-nginx:
      enabled: true
  dns:
    external-dns:
      enabled: true
  logging:
    loki:
      enabled: true
    promtail:
      enabled: true
  certificates:
    cert-manager:
      enabled: true
    qovery-cert-manager-webhook:
      enabled: true
    cert-manager-configs:
      enabled: true
  observability:
    metrics-server:
      enabled: true

## Qovery Common config
# Past information from Qovery cluster console creation

qovery:
  clusterId: &clusterId "set-by-customer"
  clusterShortId: &shortClusterId "set-by-customer"
  organizationId: &organizationId "set-by-customer"
  jwtToken: &jwtToken "set-by-customer"
  domain: &domain "set-by-customer"
  grpcServer: &grpcServer "set-by-customer"
  engineGrpcServer: &engineGrpcServer "set-by-customer"
  qoveryDnsUrl: &qoveryDnsUrl "set-by-customer"
  lokiUrl: &lokiUrl "set-by-customer"
  promtailLokiUrl: &promtailLokiUrl "set-by-customer"
  acmeEmailAddr: &acmeEmailAddr "set-by-customer"
  externalDnsPrefix: &externalDnsPrefix "set-by-customer"
  architectures: &architectures "set-by-customer"

## Chart overrides
...
```

</details>

</li>

<li>

Install Qovery on your Kubernetes cluster.

```bash
helm upgrade --install -n qovery -f values-demo.yaml qovery
```
* `-n qovery`: the namespace where Qovery and its dependencies will be installed
* `-f values-demo.yaml`: specify the values overrides file you want to use
* `qovery`: name of the chart to deploy

</li>

</ol>

</Steps>

## Configuration

### Qovery

This is the configuration of Qovery itself. It is used by all Qovery components.

<Alert type="danger">

**Do not share the jwtToken! Keep it in a safe place.** It is used to authenticate the cluster.

</Alert>

| Key                        | Required | Description                                                    | Default                   |
|----------------------------|----------|----------------------------------------------------------------|---------------------------|
| `qovery.clusterId`         | Yes      | The cluster ID. It is used to identify your cluster.           | `set-by-customer`         |
| `qovery.shortClusterId`    | Yes      | The short cluster ID. It is used to identify your cluster.     | `set-by-customer`         |
| `qovery.organizationId`    | Yes      | The organization ID. It is used to identify your organization. | `set-by-customer`         |
| `qovery.jwtToken`          | Yes      | The JWT token. It is used to authenticate your cluster.        | `set-by-customer`         |
| `qovery.domain`            | Yes      | The domain name used by Qovery.                                | `set-by-customer`         |
| `qovery.qoveryDnsUrl`      | Yes      | Qovery DNS url in case you want to use Qovery provided DNS     | `set-by-customer`         |
| `qovery.lokiUrl`           | No       | Local Loki URL (required if Loki is set)                       | `set-by-customer`         |
| `qovery.promtailLokiUrl`   | No       | Promtail Loki URL (required if Promtail and Loki are set)      | `set-by-customer`         |
| `qovery.acmeEmailAddr`     | No       | Email address used for `Let's Encrypt` TLS requests            | `set-by-customer`         |
| `qovery.externalDnsPrefix` | No       | ExernalDNS TXT record prefix (required if ExternalDNS is set)  | `set-by-customer`         |
| `qovery.architectures`     | No       | Set cluster architectures (comma separated)                    | `AMD64`                   |

#### Qovery Cluster Agent

| | | 
|-----------------|----------|
| **Required**    | Yes      |
| **If deployed** | The cluster agent is responsible for securely forwarding logs and metrics from your Kubernetes cluster to Qovery control plane |
| **If missing**  | The cluster will not report to Qovery control plane Kubernetes information, so the Qovery console will report unknown satus values |


```yaml
qovery-cluster-agent:
  fullnameOverride: qovery-cluster-agent
```

#### Qovery Shell Agent

| | | 
|-----------------|-----|
| **Required**    | Yes |
| **If deployed** | Used to give a remote shell access to you Kubernetes pods (if user is allowed from Qovery RBAC) with the Qovery CLI |
| **If missing**  | No remote connection will be possible, and Qovery support will not be able to help you to diagnose issues |

```yaml
qovery-shell-agent:
  fullnameOverride: qovery-shell-agent
```

### Ingress

| | | 
|-----------------|-------------------------------|
| **Required**    | No (but strongly recommended) |
| **If deployed** | Web services can be privately or publicly exposed |
| **If missing**  | No web services will be exposed |

Qovery us will be exposed [NGINX Ingress Controller](https://docs.nginx.com/nginx-ingress-controller/) by default to route traffic to your applications.

#### Nginx Ingress Controller

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"demo"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"NginxIngress","label":"Demo","value":"demo"},
    {"group":"NginxIngress","label":"AWS","value":"aws"},
    {"group":"NginxIngress","label":"GCP","value":"gcp"},
    {"group":"NginxIngress","label":"Scaleway","value":"scaleway"},
  ]}>

<TabItem value="demo">

Here is the minimum override configuration to be used:

```yaml
ingress-nginx:
  fullnameOverride: ingress-nginx
  controller:
    useComponentLabel: true
    admissionWebhooks:
      enabled: false
    # Ingress class used when an application/container with public access is set
    ingressClass: nginx-qovery
    extraArgs:
      # Default TLS certificate name and path
      default-ssl-certificate: "qovery/letsencrypt-acme-qovery-cert"
    # Allows customization of the source of the IP address or FQDN to report in the ingress status field
    publishService:
      enabled: true
```
</TabItem>

<TabItem value="aws">

Here is an example with Nginx Ingress Controller on AWS with NLB:

```yaml 
ingress-nginx:
  controller:
    useComponentLabel: true
    admissionWebhooks:
      enabled: set-by-customer
    # enable if you want metrics scrapped by prometheus
    metrics:
      enabled: set-by-customer
      serviceMonitor:
        enabled: set-by-customer
    config:
      # set global default file size limit to 100m
      proxy-body-size: 100m
      # hide Nginx version
      server-tokens: "false"
    # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
    ingressClass: nginx-qovery
    extraArgs:
      # Kubernetes path of the default Cert-manager TLS certificate (if used)
      default-ssl-certificate: "cert-manager/letsencrypt-acme-qovery-cert"
    updateStrategy:
      rollingUpdate:
        # set the minimum acceptable number of unavailable pods during a rolling update
        maxUnavailable: 1
    # enable auoscaling if you want to scale the number of replicas based on CPU usage
    autoscaling:
      enabled: true
      minReplicas: set-by-customer
      maxReplicas: set-by-customer
      targetCPUUtilizationPercentage: set-by-customer
    # required if you rely on a load balancer
    # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
    publishService:
      enabled: true
    # set a load balancer if you want your Nginx to be publicly accessible
    service:
      enabled: true
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: nlb
        # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
        external-dns.alpha.kubernetes.io/hostname: "set-by-customer"
      externalTrafficPolicy: "Local"
      sessionAffinity: ""
      healthCheckNodePort: 0
```

</TabItem>

<TabItem value="gcp">

Here is an example with Nginx Ingress Controller on AWS with NLB:

```yaml 
ingress-nginx:
  controller:
    useComponentLabel: true
    admissionWebhooks:
      enabled: set-by-customer
    # enable if you want metrics scrapped by prometheus
    metrics:
      enabled: set-by-customer
      serviceMonitor:
        enabled: set-by-customer
    config:
      # set global default file size limit to 100m
      proxy-body-size: 100m
      # hide Nginx version
      server-tokens: "false"
    # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
    ingressClass: nginx-qovery
    extraArgs:
      # Kubernetes path of the default Cert-manager TLS certificate (if used)
      default-ssl-certificate: "qovery/letsencrypt-acme-qovery-cert"
    updateStrategy:
      rollingUpdate:
        # set the minimum acceptable number of unavailable pods during a rolling update
        maxUnavailable: 1
    # enable auoscaling if you want to scale the number of replicas based on CPU usage
    autoscaling:
      enabled: true
      minReplicas: set-by-customer
      maxReplicas: set-by-customer
      targetCPUUtilizationPercentage: set-by-customer
    # required if you rely on a load balancer
    # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
    publishService:
      enabled: true
    # set a load balancer if you want your Nginx to be publicly accessible
    service:
      enabled: true
      annotations:
        # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
        external-dns.alpha.kubernetes.io/hostname: "set-by-customer"
      externalTrafficPolicy: "Local"
      sessionAffinity: ""
      healthCheckNodePort: 0
```

</TabItem>

<TabItem value="scaleway">

Here is an example with Nginx Ingress Controller on Scaleway:

```yaml
ingress-nginx:
  controller:
    useComponentLabel: true
    admissionWebhooks:
      enabled: set-by-customer
    # enable if you want metrics scrapped by prometheus
    metrics:
      enabled: set-by-customer
      serviceMonitor:
        enabled: set-by-customer
    config:
      # set global default file size limit to 100m
      proxy-body-size: 100m
      # hide Nginx version
      server-tokens: "false"
      # required for X-Forwarded-for to work
      use-proxy-protocol: "true"
    # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
    ingressClass: nginx-qovery
    extraArgs:
      # Kubernetes path of the default Cert-manager TLS certificate (if used)
      default-ssl-certificate: "cert-manager/letsencrypt-acme-qovery-cert"
    updateStrategy:
      rollingUpdate:
        # set the minimum acceptable number of unavailable pods during a rolling update
        maxUnavailable: 1
    # enable auoscaling if you want to scale the number of replicas based on CPU usage
    autoscaling:
      enabled: true
      minReplicas: set-by-customer
      maxReplicas: set-by-customer
      targetCPUUtilizationPercentage: set-by-customer
    # required if you rely on a load balancer
    # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
    publishService:
      enabled: true
    # set a load balancer if you want your Nginx to be publicly accessible
    service:
      enabled: true
      # https://github.com/scaleway/scaleway-cloud-controller-manager/blob/master/docs/loadbalancer-annotations.md
      annotations:
        service.beta.kubernetes.io/scw-loadbalancer-forward-port-algorithm: "leastconn"
        service.beta.kubernetes.io/scw-loadbalancer-protocol-http: "false"
        service.beta.kubernetes.io/scw-loadbalancer-proxy-protocol-v1: "false"
        service.beta.kubernetes.io/scw-loadbalancer-proxy-protocol-v2: "true"
        service.beta.kubernetes.io/scw-loadbalancer-health-check-type: tcp
        service.beta.kubernetes.io/scw-loadbalancer-use-hostname: "true"
        # set Scaleway load balancer type https://www.scaleway.com/en/load-balancer/ (ex: LB-GP-S, LB-GP-M, LB-GP-L, LB-GP-XL)
        service.beta.kubernetes.io/scw-loadbalancer-type: "set-by-customer"
        # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
        external-dns.alpha.kubernetes.io/hostname: "set-by-customer"
      externalTrafficPolicy: "Local"
```

</TabItem>

</Tabs>

#### Other Ingress Controllers

Qovery supports other Ingress Controllers. Please contact us if you want to use another one. We will be happy to help you.

### DNS

| | | 
|-----------------|-------------------------------|
| **Required**    | No (but strongly recommended) |
| **If deployed** | Used to easily reach your applications with DNS records, even on private network |
| **If missing**  | You will have easy access with dns names to your services, you'll have to use IPs |

Qovery uses [External DNS](https://github.com/kubernetes-sigs/external-dns) to automatically configure DNS records for your applications.

If you don't want or can't add your own DNS provider, Qovery proposes it's own managed sub-domain DNS provider for free.
You'll then be able to later add your custom DNS record (no matter the provider) to point to your Qovery DNS sub-domain.

#### External DNS

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"demo"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"ExternalDns","label":"Demo & QoveryDNS","value":"demo"},
    {"group":"ExternalDns","label":"Cloudflare","value":"cloudflare"},
  ]}>

<TabItem value="demo">

Here is one example with Qovery DNS provider:
```yaml
external-dns:
  fullnameOverride: external-dns
  # set pdns for Qovery DNS managed (or you can use any supported provider by external-dns)
  provider: pdns
  # will use the domain name given by Qovery during the cluster setup phease
  domainFilters: [*domain]
  # an owner ID is set to avoid conflicts in case of multiple Qovery clusters
  txtOwnerId: *shortClusterId
  # a prefix to help Qovery to debug in case of issues
  txtPrefix: *externalDnsPrefix
  # set the Qovery DNS provider configuration
  pdns:
    apiUrl: *qoveryDnsUrl
    apiKey: *jwtToken
    apiPort: 443
```

</TabItem>

<TabItem value="cloudflare">

Here is one example with Cloudflare:
```yaml
external-dns:
  # set the provider to use
  provider: set-by-customer
  # keep the config you want to use and remove the others. Configure the provider you want to use.
  cloudflare:
    apiToken: set-by-customer
    email: set-by-customer
    proxied: set-by-customer
  pdns:
    # Qovery DNS: apiUrl: *qoveryDnsUrl
    apiUrl: set-by-customer
    # Qovery DNS: apiPort: "443"
    apiPort: set-by-customer
    # Qovery DNS: apiKey: "443"
    apiKey: set-by-customer
  # Make external DNS ignore this ingress https://github.com/kubernetes-sigs/external-dns/issues/1910#issuecomment-976371247
  annotationFilter: external-dns.alpha.kubernetes.io/exclude notin (true)
  # set domainFilters to the domain you want to manage: [*domain]
  domainFilters: set-by-customer
  triggerLoopOnEvent: true
  policy: sync
  # avoid dns collision with other external-dns instances
  txtOwnerId: set-by-customer
  txtPrefix: set-by-customer
  # set the number of replicas you want to use
  replicas: 1
  # set the rolling update strategy you want to apply
  updateStrategy:
    type: set-by-customer
  # remove if you don't want to use a custom image
  image:
    registry: set-by-customer
    repository: set-by-customer
    tag: 0.13.2-debian-11-r17
  # set resources
  resources:
    limits:
      cpu: 50m
      memory: 100Mi
    requests:
      cpu: 50m
      memory: 100Mi
```

</TabItem>

</Tabs>

### Logging

| | | 
|-----------------|-------------------------------|
| **Required**    | No (but strongly recommended) |
| **If deployed** | Retrieve and store application's log history |
| **If missing**  | You'll have live logs, but you will miss log history for debugging purpose |

Qovery uses [Loki](https://grafana.com/oss/loki/) to store your logs in a S3 compatible bucket and [Promtail](https://grafana.com/docs/loki/latest/clients/promtail/) to collect your logs.

#### Loki
<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"demo"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"Loki","label":"Demo","value":"demo"},
    {"group":"Loki","label":"AWS S3","value":"s3"},
  ]}>

<TabItem value="demo">

Here is a configuration **in Memory (no persistence)** for Loki:

```yaml
loki:
  fullnameOverride: loki
  loki:
    # no auth is set for internal cluster usage
    auth_enabled: false
    ingester:
      lifecycler:
        ring:
          kvstore:
            # we store it in memory for the demo, you'll lose history once Loki restarts
            store: inmemory
          replication_factor: 1
    schema_config:
      configs:
        - from: 2020-05-15
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
  monitoring:
    # all the monitoring part is disabled to reduce resource footprint for the demo usage
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
      metricsInstance:
        enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    grafanaAgent:
      enabled: false
    lokiCanary:
      enabled: false
  test:
    enabled: false
  gateway:
    enabled: false
  # we use a single binary to reduce resource footprint for the demo usage
  singleBinary:
    replicas: 1
    persistence:
      enabled: false
    extraVolumes:
      - name: data
        emptyDir: {}
      - name: storage
        emptyDir: {}
    extraVolumeMounts:
      - name: data
        mountPath: /data
      - name: storage
        mountPath: /var/loki
```

</TabItem>

<TabItem value="s3">

Here is a configuration example with AWS S3 as storage backend:

```yaml
loki:
  # remove if you don't want to use a custom image
  kubectlImage:
    registry: set-by-customer
    repository: set-by-customer
  loki:
    # remove if you don't want to use a custom image
    image:
      registry: set-by-customer
      repository: set-by-customer
    # set if you want to use authentication
    auth_enabled: false
    commonConfig:
      # for simple usage, without high throughput, you can use the 1 replica only
      # note: replication is assured by the storage backend
      replication_factor: 1
    ingester:
      chunk_idle_period: 3m
      chunk_block_size: 262144
      chunk_retain_period: 1m
      max_transfer_retries: 0
      lifecycler:
        ring:
          kvstore:
            store: memberlist
          replication_factor: 1
    memberlist:
      abort_if_cluster_join_fails: false
      bind_port: 7946
      join_members:
        # set loki headless service
        - loki-headless.logging.svc:7946
      max_join_backoff: 1m
      max_join_retries: 10
      min_join_backoff: 1s
    limits_config:
      ingestion_rate_mb: 20
      ingestion_burst_size_mb: 30
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_concurrent_tail_requests: 100
      split_queries_by_interval: 15m
      max_query_lookback: 12w
    compactor:
      working_directory: /data/retention
      # configure storage provider for the compactor
      shared_store: aws
      compaction_interval: 10m
      retention_enabled: set-by-customer
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
    table_manager:
      retention_deletes_enabled: set-by-customer
      retention_period: set-by-customer
    schema_config:
      configs:
        # set the schema for the index (2020 version can be deleted on a fresh install)
        - from: 2020-05-15
          store: boltdb-shipper
          object_store: s3
          schema: v11
          index:
            prefix: index_
            period: 24h
        - from: 2023-06-01
          store: boltdb-shipper
          object_store: s3
          schema: v12
          index:
            prefix: index_
            period: 24h
    storage:
      # configure the object storage backend
      bucketNames:
        chunks:
        ruler:
        admin:
      type: s3
      s3:
        s3:
        region:
        s3ForcePathStyle:
        insecure:
    storage_config:
      boltdb_shipper:
        active_index_directory: /data/loki/index
        shared_store: s3
        resync_interval: 5s
        cache_location: /data/loki/boltdb-cache
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
      metricsInstance:
        enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    grafanaAgent:
      enabled: false
    lokiCanary:
      enabled: false
  test:
    enabled: false
  gateway:
    enabled: false
  # set the single binary version for basic usage
  singleBinary:
    replicas: 1
    # set resources
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 300m
        memory: 1Gi
    persistence:
      enabled: false
    extraVolumes:
      - name: data
        emptyDir: {}
      - name: storage
        emptyDir: {}
    extraVolumeMounts:
      - name: data
        mountPath: /data
      - name: storage
        mountPath: /var/loki
    # set disk persistence to reduce data loss in case of pod crash
    # persistence:
    #   storageClass: set-by-customer
  serviceAccount:
    annotations: {}
```

</TabItem>

</Tabs>

#### Promtail

A configuration example compatible with all providers:

```yaml
promtail:
  fullnameOverride: promtail
  # promtail requires to be spawned in kube-system namespace
  namespace: kube-system
  priorityClassName: system-node-critical
  config:
    clients:
      # forward logs to Loki
      - url: *promtailLokiUrl
    snippets:
      extraRelabelConfigs:
        - action: labelmap
          # required to be able to watch logs from Qovery console interface
          regex: __meta_kubernetes_pod_label_(qovery_com_service_id|qovery_com_service_type|qovery_com_environment_id)
```

### Certificates

| | | 
|-----------------|-------------------------------|
| **Required**    | No (but strongly recommended) |
| **If deployed** | Cert-manager helps you to get TLS certificates through Let's Encrypt |
| **If missing**  | Without it, you will not be able to automatically get TLS certificates |

Qovery uses [Cert Manager](https://cert-manager.io/) to automatically get TLS certificates for your applications.

#### Cert Manager

Here is the minimal setup for all cloud providers:

```yaml
cert-manager:
  fullnameOverride: cert-manager
  # CRD are required
  installCRDs: true
  replicaCount: 1
  startupapicheck:
    jobAnnotations:
      helm.sh/hook: post-install,post-upgrade
    rbac:
      annotations:
        helm.sh/hook: post-install,post-upgrade
    serviceAccount:
      annotations:
        helm.sh/hook: post-install,post-upgrade
```

#### Qovery Cert Manager Webhook

| | | 
|-----------------|----------------------------------------------|
| **Required**    | No (but if you're using Qovery DNS Provider) |
| **If deployed** | Required to get Let's Encrypt TLS if Qovery DNS Provider is used |
| **If missing**  | Without it, you will not be able to automatically get TLS certificates with Qovery DNS Provider |

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"qovery"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"QoveryCertManagerWebhook","label":"Qovery DNS","value":"qovery"},
    {"group":"QoveryCertManagerWebhook","label":"","value":""},
  ]}>

<TabItem value="qovery">

A configuration example compatible with all providers:

```yaml
qovery-cert-manager-webhook:
  fullnameOverride: qovery-cert-manager-webhook
  certManager:
    # set the same namespace than cert-manager
    namespace: qovery
    serviceAccountName: cert-manager
  secret:
    apiUrl: *qoveryDnsUrl
    apiKey: *jwtToken
```

</TabItem>
</Tabs>

#### Cert Manager Configs

| | | 
|-----------------|----------------------------------------------|
| **Required**    | No |
| **If deployed** | This is an helper to deploy cert-manager config. But you can manually set it |
| **If missing**  | Installing Cert-manager is not enough, you have to configure it to get TLS working |

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"demo"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"ExternalDns","label":"Demo","value":"demo"},
    {"group":"ExternalDns","label":"Qovery DNS","value":"qovery"},
    {"group":"ExternalDns","label":"Cloudflare","value":"cloudflare"},
  ]}>

<TabItem value="demo">

This is the configuration of Cert Manager itself. It is used by all Cert Manager components.

```yaml
cert-manager-configs:
  fullnameOverride: cert-manager-configs
  # set pdns to use Qovery DNS provider
  externalDnsProvider: pdns
  managedDns: [*domain]
  acme:
    letsEncrypt:
      emailReport: *acmeEmailAddr
      # As it's a demo cluster, we use the staging environment to avoid rate limit issues
      acmeUrl: https://acme-staging-v02.api.letsencrypt.org/directory
  provider:
    # set the provider of your choice or use the Qovery DNS provider
    pdns:
      apiPort: 443
      apiUrl: *qoveryDnsUrl
      apiKey: *jwtToken
```

</TabItem>

<TabItem value="qovery">

This is the configuration of Cert Manager itself. It is used by all Cert Manager components.

```yaml
cert-manager-configs:
  fullnameOverride: cert-manager-configs
  # set pdns to use Qovery DNS provider
  externalDnsProvider: pdns
  managedDns: [*domain]
  acme:
    letsEncrypt:
      emailReport: *acmeEmailAddr
      # set the Let's Encrypt URL
      # Test: https://acme-staging-v02.api.letsencrypt.org/directory
      # Prod: https://acme-v02.api.letsencrypt.org/directory
      acmeUrl: https://acme-v02.api.letsencrypt.org/directory
  provider:
    # set the provider of your choice or use the Qovery DNS provider
    pdns:
      apiPort: 443
      apiUrl: *qoveryDnsUrl
      apiKey: *jwtToken
```

</TabItem>

<TabItem value="cloudflare">

This is the configuration of Cert Manager itself. It is used by all Cert Manager components.

```yaml
cert-manager-configs:
  fullnameOverride: cert-manager-configs
  # set pdns to use Qovery DNS provider
  externalDnsProvider: cloudflare
  managedDns: [*domain]
  acme:
    letsEncrypt:
      emailReport: *acmeEmailAddr
      acmeUrl: https://acme-v02.api.letsencrypt.org/directory
  provider:
    cloudflare:
      apiToken: "set your Cloudflare API token here"
      email: "set your Cloudflare email here"
```

</TabItem>

</Tabs>

Qovery uses [Metrics Server](https://github.com/kubernetes-sigs/metrics-server) to collect metrics from your Kubernetes cluster and scale your applications automatically based on custom metrics.

## Observability

### Metrics Server

| | | 
|-----------------|-------------------------------|
| **Required**    | No (but strongly recommended) |
| **If deployed** | Mandatory if you want to retrive pod metrics for the Qovery agent and if you want to be able to use the horizontal pod scaling |
| **If missing**  | No HPA and no application metrics in the QOveyr console |

<Tabs
  centered={true}
  className={"rounded"}
  defaultValue={"demo"}
  placeholder="Select a platform"
  select={false}
  size={null}
  values={[
    {"group":"MetricsServer","label":"Demo","value":"demo"},
    {"group":"MetricsServer","label":"AWS","value":"aws"},
    {"group":"MetricsServer","label":"GCP","value":"gcp"},
    {"group":"MetricsServer","label":"Scaleway","value":"scaleway"},
  ]}>

<TabItem value="aws">

```yaml
metrics-server:
  # create api service to be able to use hpa/vpa
  apiService:
    create: true
  # set rolling restart strategy
  updateStrategy:
    type: set-by-customer
  # set resources
  resources:
    limits:
      cpu: set-by-customer
      memory: set-by-customer
    requests:
      cpu: set-by-customer
      memory: set-by-customer
```

</TabItem>

<TabItem value="gcp">

Nothing needs to be deployed, as GCP already provides a managed metrics server.

</TabItem>

<TabItem value="scaleway">

Nothing needs to be deployed, as Scaleway already provides a managed metrics server.

</TabItem>

<TabItem value="demo">

```yaml
metrics-server:
  fullnameOverride: metrics-server
  defaultArgs:
    - --cert-dir=/tmp
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --kubelet-use-node-status-port
    - --metric-resolution=15s
    - --kubelet-insecure-tls
  apiService:
    create: false
```

</TabItem>
</Tabs>

## FAQ

### I have a non-covered use case. What should I do?

Please [contact us][urls.qovery_contact_us]. We will be happy to help you.

### Can I host the Qovery control plane on my own?

At the momement, you can't. But please [contact us][urls.qovery_contact_us] to discuss about it. We will be happy to help you.


[docs.using-qovery.configuration.cloud-service-provider.amazon-web-services]: /docs/using-qovery/configuration/cloud-service-provider/amazon-web-services/
[docs.using-qovery.configuration.cloud-service-provider.google-cloud-platform]: /docs/using-qovery/configuration/cloud-service-provider/google-cloud-platform/
[docs.using-qovery.configuration.cloud-service-provider.microsoft-azure]: /docs/using-qovery/configuration/cloud-service-provider/microsoft-azure/
[docs.using-qovery.deployment.image-mirroring]: /docs/using-qovery/deployment/image-mirroring/
[guides.provider.guide-kubernetes]: /guides/provider/guide-kubernetes/
[urls.helm]: https://helm.sh
[urls.qovery_contact_us]: https://www.qovery.com/contact
